<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RheoAgent: A Cross-Rheological Material Handling Robotic Manipulation System via Hierarchical Decision-Making Framework">
  <meta name="keywords" content="Rheological materials, Rheological heterogeneity, Manipulation, Large Vision-Language Model, Reinforcement learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RheoAgent: A Cross-Rheological Material Handling Robotic Manipulation System via Hierarchical Decision-Making Framework</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RheoAgent: A Cross-Rheological Material Handling Robotic Manipulation System via Hierarchical Decision-Making Framework</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Haixu Zhang</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Bo Zhang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Danyang Zhang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Weiqiang Lai</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Xi Chen</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Tin Lun Lam</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Hu Huang</a><sup>1,2</sup>
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Yuan Gao></a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen</span>
            <span class="author-block"><sup>2</sup>Shenzhen Institute of Artificial Intelligence and Robotics for Society</span>
            <span class="author-block"><sup>3</sup>Shenzhen City Joint Laboratory of Autonomous Unmanned Systems and Intelligent Manipulation</span>
            <span class="author-block"><sup>4</sup>Beijing Institute for General Artificial Intelligence</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948111"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948111"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA111"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies111"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1111"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- 修复图片标签 -->
      <img id="teaser" src="./static/images/schematic.png" alt="Schematic image" style="width: 100%; height: auto;">
      
      <h2 class="subtitle has-text-justified">
        <p class="bold-text" style="display: inline;">Fig. 1: Capabilities of the RheoAgent framework.</p> RheoAgent is an advanced hybrid framework integrating LVLMs and RL for effective manipulation of cross-rheological materials. By leveraging multi-modal inputs (visual and haptic), it classifies unknown rheological materials and incorporates them into a differentiable environment, optimizing reinforcement learning via differentiable RL (DRL) to adapt dynamically to rheological heterogeneity. Panel (a-f) depicts a robotic arm executing a pouring task with rheological materials: <strong>a</strong>, LVLM identifies rheological material types (granular and fluid). <strong>b</strong>, Successfully pouring granular materials using a pre-trained policy. <strong>c</strong>, An imagined scenario of pouring failure for fluid materials if using the same pre-trained policy. <strong>d</strong>, Shaking the fluid container with force feedback to measure the density and viscosity range of fluid materials. <strong>e</strong>, Fine-tuning the pre-trained policy based on identified fluid parameters. <strong>f</strong>, Successfully pouring fluid materials using the fine-tuned policy. <strong>g</strong>, Demonstrates the robot manipulating diverse rheological materials in a kitchen environment, including granular substances, viscous fluids, inviscous fluids, and elastoplastic materials.
      </h2>
    </div>
  </div>
</section>



<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic manipulation in environments such as biochemical laboratories and kitchens faces challenges due to the rheological heterogeneity of materials, affecting their mechanical response to external forces. Traditional approaches often focus on manipulating materials of a single rheological type, neglecting how operational strategies must adapt when handling materials with varying flow and deformation characteristics. In this work, we introduce a hierarchical decision-making robotic manipulation system for cross-rheological material handling, namely RheoAgent, to tackle the robotics manipulation challenges posed by diverse rheological material scenarios. Our approach combines the material classification capabilities of large vision-language models (LVLMs) — achieving an accuracy of 86.0\% — with dual-modal vision-driven reinforcement learning (RL) using pixel map and voxel map. This integration forms a system capable of flexible operations across various rheological materials. The learning process of the RL module consists of two phases: a rapid learning process for the baseline model and a model fine-tuning process that incorporates specific rheological characteristics, effectively balancing learning costs and model accuracy. Exceeding baseline model-free RL models that have not been fine-tuned by 58.17–98.97\% points through fine-tuning with a model-based approach, the model achieves success rates of 99.08\% and 95.12\% for inviscous materials, 99.07\% and 96.98\% for viscous materials, and 98.53\% and 97.51\% for elasto-plastic materials in the pouring and gathering tasks, respectively. RheoAgent establishes a new paradigm for intelligent material handling, with applications extending to industrial automation, laboratory robotics, and assistive technologies.
          </p>
          <!-- <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Video</h2>
        <div class="columns is-vcentered">
          <!-- Left video -->
          <div class="column">
            <video controls autoplay muted loop width="100%">
              <source src="./static/videos/gather_multi.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p class="is-size-6 mt-2">Pouring Task (4× Speed)</p>
          </div>
          <!-- Right video -->
          <div class="column">
            <video controls autoplay muted loop width="100%">
              <source src="./static/videos/pour_multi.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p class="is-size-6 mt-2">Gathering Task (4× Speed)</p>
          </div>
        </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Pour. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Pouring Under Human Disturbance</h2>
          <p>
            The robot continues pouring while a human introduces interference by shifting the target cup.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/external_pour.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Pour. -->

      <!-- Gather. -->
      <div class="column">
        <h2 class="title is-3">Gathering Under Human Disturbance</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              The robot continues collecting while a human introduces interference by adding material during and after the task.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/external_gather.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Gather. -->


    <!-- Re-rendering. -->
    <h3 class="title is-3">Real-time Segmentation and Perception Mapping</h2>
    <div class="content has-text-justified">
      <p>
        We employ the <strong>Segment Anything Model 2 (SAM2)</strong> to perform real-time segmentation of rheological materials from RGB-D inputs.
        To build a consistent spatial understanding, <strong>multi-view RGB-D frames</strong> are fused into a globally aligned segmented point cloud.
        This point cloud is then encoded into two representations:
      </p>
      <ul>
        <li><strong>Pixel map</strong>: generated via orthographic projection of the segmented point cloud onto the vertical plane.</li>
        <li><strong>Voxel map</strong>: constructed by encoding spatial occupancy on a latitude-longitude grid with normalized distances.</li>
      </ul>
      <p>
        These representations serve as structured perceptual inputs for downstream robotic manipulation policies.
      </p>
    </div>
    <div class="content has-text-centered">
      <video id="replay-video"
              controls
              muted
              preload
              playsinline
              width="75%">
        <source src="./static/videos/perception.mp4"
                type="video/mp4">
      </video>
    </div>
    <!--/ Re-rendering. -->

    <!-- Re-rendering. -->
    <h3 class="title is-3">Multi-Robot Collaboration: Collection</h2>
      <div class="content has-text-justified">
        <p>
          Two robots cooperate in a gathering task: the dual-arm Elephant Mercury B1 pushes material with a plate, while the UR5 collects it using a cup.
        </p>
      </div>
      <div class="content has-text-centered">
        <video id="replay-video"
                controls
                muted
                preload
                playsinline
                width="75%">
          <source src="./static/videos/multi-agent.mp4"
                  type="video/mp4">
        </video>
      </div>
      <!--/ Re-rendering. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <!-- <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{haixu2025nmi,
  author    = {Haixu Zhang, Bo Zhang, Danyang Zhang, Xi Chen, Wenqiang Lai, Chi Zhang, Tin Lun Lam, Hu Huang, Yuan Gao},
  title     = {RheoAgent: A Cross-Rheological Material Handling Robotic Manipulation System via Hierarchical Decision-Making Framework},
  journal   = {NMI},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
